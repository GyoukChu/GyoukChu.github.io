---
layout: single
title:  "[Coding] HuggingFace Accelerate"
categories: Coding
tag: [Coding]
author_profile: false
---

torch.distributed packageì—ì„œ ì œê³µí•˜ëŠ” Multi-GPU trainingì— ëŒ€í•´ ì½”ë“œë¥¼ ë³´ë©´ì„œ ì•Œì•„ë³´ì. Accelerate/Ignite/Lightning ë“±ì˜ libraryëŠ” ì´ëŸ° distributed runì„ ì¢€ ë” ì‰½ê²Œ í•´ì£¼ëŠ”ë°, ì´ ì¤‘ HuggingFaceì˜ Accelerateë¥¼ í™œìš©í•œ ê²ƒë„ ì½”ë“œë¥¼ ë³´ë©´ì„œ ì•Œì•„ë³´ì.

# 0. Single GPU

PyTorch ê³µì‹ì—ë„ ì˜¬ë¼ì™€ìˆëŠ” MNIST ë¶„ë¥˜ ëª¨ë¸ ì½”ë“œì´ë‹¤. CUDA:0ìœ¼ë¡œ GPU í•˜ë‚˜ ì‚¬ìš©í•´ì„œ í•™ìŠµí•˜ëŠ”ë°, ì•„ë˜ì—ì„œ ë™ì¼í•˜ê²Œ Multi-GPUë¡œ í•™ìŠµí•´ë³´ê¸° ì „ baselineì´ë‹¤.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class BasicNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.act = F.relu

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.act(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.act(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

def main():
    device = "cuda"

    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307), (0.3081))
    ])

    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)
    test_dset = datasets.MNIST('data', train=False, transform=transform)

    train_loader = DataLoader(train_dset, shuffle=True, batch_size=64)
    test_loader = DataLoader(test_dset, shuffle=False, batch_size=64)


    model = BasicNet().to(device)

    optimizer = optim.AdamW(model.parameters(), lr=1e-3)

    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    print(f'Accuracy: {100. * correct / len(test_loader.dataset)}')

if __name__ == "__main__":
    main()
```

# 1. DP vs DDP

Pytorch Distributed Overviewì— ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì´ ë‚˜ì˜¨ë‹¤.

>Use single-machine multi-GPU DataParallel to make use of multiple GPUs on a single machine to speed up training with minimal code changes. Use single-machine multi-GPU DistributedDataParallel, if you would like to further speed up training and are willing to write a little more code to set it up.

torch.nn.DataParallel (DP)ì™€ torch.nn.parallel.DistributedDataParallel (DDP)ì„ ì£¼ë¡œ ì“°ëŠ”ë°, DDPê°€ DPì— ë¹„í•´ ì„¸íŒ…í•  ê²ƒë“¤ì´ ì¡°ê¸ˆ ë” ìˆë‹¤. ~~ë²ˆê±°ë¡­ë‹¤~~ ê¸°ë³¸ì ìœ¼ë¡œ DPëŠ” single-process, multi-threadì— single nodeë§Œ ì§€ì›í•œë‹¤. ê·¸ëŸ¬ë‚˜ Pythonì˜ Global Interpreter Lockì— ì˜í•´ì„œ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì—¬ëŸ¬ ê°œì˜ threadê°€ ì‘ë™í•  ìˆ˜ ì—†ê¸°ì—, ê·¼ë³¸ì ìœ¼ë¡œëŠ” multi-process í”„ë¡œê·¸ë¨ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ ëœë‹¤. ê·¸ë¦¬ê³  forward ì—°ì‚° ì‹œ í•˜ë‚˜ì˜ GPUì— ëª¨ë“  ë‹¤ìŒì— lossë¥¼ ê³„ì‚°í•´ì„œ, 0ë²ˆ GPUì— memoryê°€ ë” ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¬¸ì œì ì´ ìˆë‹¤.

ìœ„ì™€ ë™ì¼í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ë©´ì„œ, DPë¡œ Multi-GPU trainingì„ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. GPU 6ë²ˆ 7ë²ˆ 2ê°œë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

```python
    import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class BasicNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.act = F.relu

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.act(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.act(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

def main():
    device = "cuda:6"

    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307), (0.3081))
    ])

    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)
    test_dset = datasets.MNIST('data', train=False, transform=transform)

    train_loader = DataLoader(train_dset, shuffle=True, batch_size=64)
    test_loader = DataLoader(test_dset, shuffle=False, batch_size=64)

    model = BasicNet().to(device="cuda:6")
    model = nn.DataParallel(model, device_ids=[6, 7], output_device=6)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)

    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.detach().cpu().eq(target.view_as(pred)).sum().item()
    print(f'Accuracy: {100. * correct / len(test_loader.dataset)}')

if __name__ == "__main__":
    main()
```

DDPë¡œ Multi-GPU trainingì„ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import argparse
import torch.distributed as dist
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP

from MySampler import DistributedEvalSampler
# See: https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py

class BasicNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.act = F.relu

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.act(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.act(self.fc1(x))
        x = self.dropout2(x)
        output = self.fc2(x)
        return output

def main(opts):
    # init dist
    init_for_distributed(opts)
    local_gpu_id = opts.gpu

    # data set
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307), (0.3081))
        ])

    train_set = datasets.MNIST('data', train=True, download=True, transform=transform)
    test_set = datasets.MNIST('data', train=False, transform=transform)

    train_sampler = DistributedSampler(dataset=train_set, shuffle=True)
    '''
    https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py

    DistributedEvalSampler is different from DistributedSampler.
    It does NOT add extra samples to make it evenly divisible.
    DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever.
    See this issue for details: https://github.com/pytorch/pytorch/issues/22584
    '''
    test_sampler = DistributedEvalSampler(dataset=test_set, shuffle=False)
    # test_sampler2 = DistributedSampler(dataset=test_set, shuffle=False)

    train_loader = DataLoader(dataset=train_set,
                              batch_size=int(64 / opts.world_size), # batch_size=64
                               shuffle=False,
                               num_workers=int(opts.num_workers / opts.world_size),
                               sampler=train_sampler,
                               pin_memory=True)
    test_loader = DataLoader(dataset=test_set,
                             batch_size=int(64 / opts.world_size),
                             shuffle=False,
                             num_workers=int(opts.num_workers / opts.world_size),
                             sampler=test_sampler,
                             pin_memory=True)

    # model
    model = BasicNet()
    model = model.cuda(local_gpu_id)
    model = DDP(module=model,
                device_ids=[local_gpu_id])

    # criterion
    criterion = torch.nn.CrossEntropyLoss().to(local_gpu_id)

    # optimizer
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)

    # scheduler
    scheduler = StepLR(optimizer=optimizer, step_size=30, gamma=0.1)
    
    for epoch in range(1): # training for one epoch
        # train
        model.train()
        '''
        In distributed mode, calling the set_epoch() method at the beginning of each epoch 
        before creating the DataLoader iterator is necessary to make shuffling work properly 
        across multiple epochs. Otherwise, the same ordering will be always used.
        '''
        train_sampler.set_epoch(epoch)

        for _, (data, target) in enumerate(train_loader):
            data, target = data.to(local_gpu_id), target.to(local_gpu_id)
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        # test
        model.eval()

        correct = 0
        with torch.no_grad():
            for _, (data, target) in enumerate(test_loader):
                data = data.to(opts.rank)
                labels = target.to(opts.rank)
                outputs = model(data)
                pred = outputs.argmax(dim=1, keepdim=True)
                correct += pred.eq(labels.view_as(pred)).sum().item()
        correct = torch.Tensor([correct]).to(opts.rank)
        global_result = [torch.zeros_like(correct) for _ in range(opts.world_size)]
        
        
        if opts.rank == 0:
            dist.gather(correct, gather_list=global_result)
        else:
            dist.gather(correct, dst=0)

        if opts.rank == 0:
            global_result_tensor = torch.cat(global_result)
            global_correct = torch.sum(global_result_tensor)
            #print(train_sampler.total_size)
            #print(test_sampler.total_size)
            #print(test_sampler2.total_size)
            print(f'Accuracy: {100. * global_correct / len(test_loader.dataset)}')
            
            scheduler.step()

    cleanup()
    return


def init_for_distributed(opts):

    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        opts.rank = int(os.environ["RANK"])
        opts.world_size = int(os.environ['WORLD_SIZE'])
        opts.gpu = int(os.environ['LOCAL_RANK'])

    torch.cuda.set_device(opts.gpu)

    #os.environ['MASTER_ADDR'] = 'localhost'
    #os.environ['MASTER_PORT'] = '23456'

    dist.init_process_group(backend='nccl', 
                            world_size=opts.world_size, 
                            rank=opts.rank)

    dist.barrier()
    setup_for_distributed(opts.rank == 0)


def setup_for_distributed(is_master):
    """
    Disable printing when not in master process
    """
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print

def cleanup():
    dist.destroy_process_group()

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--rank', type=int, default=0)
    parser.add_argument('--gpu_ids', nargs="+", default=['1', '2', '3', '4', '5', '6', '7'])
    args = parser.parse_args()

    os.environ["OMP_NUM_THREADS"] = "1" # Single Thread
    os.environ["CUDA_VISIBLE_DEVICES"] = ','.join([str(v) for v in args.gpu_ids])

    args.world_size = len(args.gpu_ids)
    args.num_workers = len(args.gpu_ids) * 4
    main(args)
```

*ì„œë²„ì—ì„œ Docker container ìƒì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ”ë°, MASTER_ADDRì´ë‘ MASTER_PORT ë¶€ë¶„ì„ ì£¼ì„ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ë‹ˆ port ì ‘ì†ì´ ì•ˆ ëœë‹¤ëŠ” ì˜¤ë¥˜ê°€ ê³„ì† ë– ì„œ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ì˜€ë‹¤. ë„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì‹œì— í¬íŠ¸ í¬ì›Œë”© ê´€ë ¨ ì˜µì…˜ì„ ë„£ì–´ì£¼ì–´ì•¼ ë˜ëŠ” ê²ƒ ê°™ì€ë°, ê´€ë¦¬ìê°€ ì•„ë‹ˆë¼ì„œ ëª»í•œë‹¤ëŠ”...*

ì´í›„ í„°ë¯¸ë„ ì°½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•´ì£¼ë©´ ëœë‹¤. "python íŒŒì¼ëª…" ì´ë ‡ê²Œ ì‹¤í–‰í•˜ë©´ ì•ˆë˜ê³ , ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¼ torchrunì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. nproc_per_node 7ëŠ” GPU 7ê°œë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì´ê³ , nnodes 1ì€ single nodeë¼ëŠ” ê²ƒì´ë‹¤.
>torchrun --nproc_per_node=7 --nnodes=1 example_script.py

## Cautions

0. DP ì‚¬ìš©ì‹œ, modelê³¼ dataëŠ” ê°€ì¥ ì²« ë²ˆì§¸ GPUì— ìˆì–´ì•¼ ëœë‹¤.
1. num_workersëŠ” GPU ëŒ€ìˆ˜ë‹¹ 4ê°€ ì ì ˆ. ë¬¼ë¡  ì´ ë˜í•œ Hyperparameterê°€ ë  ìˆ˜ ìˆê¸´ í•˜ë‹¤.
2. Process Group init ì‹œ Distributed **GPU** trainingì„ ìœ„í•´ì„œëŠ” backendë¡œ "nccl"ì„ ì‚¬ìš©í•œë‹¤.
3. DataLoader ì‹œ Batch_sizeëŠ” ì›í•˜ëŠ” batch sizeë¥¼ gpu ëŒ€ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì¤€ ìˆ˜ë¥¼ ë„£ê³ , SamplerëŠ” torch.utils.data.distributed.DistributedSamplerë¥¼ ì‚¬ìš©í•œë‹¤.
4. Evaluationì„ ìœ„í•´ì„œëŠ” dist.gather(ë˜ëŠ” all_gather ë“±)ë¥¼ ì´ìš©í•œë‹¤. ê·¸ë¦¬ê³  rank==0 ì¡°ê±´ì„ ì´ìš©í•´ì„œ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ëŠ”ë°, ì´ ë•Œ DistributedSamplerë¥¼ ì¡°ì‹¬í•´ì•¼ ëœë‹¤. ê°€ë ¹ MNISTì˜ ê²½ìš° training dataset 6ë§Œ ê°œ, test dataset 1ë§Œ ê°œì¸ë° GPUë¥¼ 7ê°œ ì‚¬ìš© ì‹œ 7ë¡œ ê°œìˆ˜ê°€ ë‚˜ëˆ ì§€ê¸° ìœ„í•´ extra sampleë¥¼ ë”í•´ì¤€ë‹¤. (drop_last íŒŒë¼ë¯¸í„°ë¥¼ Trueë¡œ ì£¼ë©´ sampleë¥¼ ëª‡ ê°œ ë¹¼ì§€ë§Œ, ì—¬ì „íˆ 7ë¡œ ë‚˜ëˆ„ì–´ ì§€ê²Œ í•œë‹¤.) ê·¸ë˜ì„œ ìœ„ ì½”ë“œì—ì„œ print(test_sampler2.total_size)ì˜ ê²°ê³¼ëŠ” 1ë§Œì´ ì•„ë‹ˆë¼ 10003ì´ ëœë‹¤. ê·¸ë˜ì„œ ì •í™•í•œ ê²°ê³¼ê°€ ì•„ë‹ˆê²Œ ë˜ëŠ”ë°, ë”°ë¼ì„œ [[DistributedEvalSampler]](https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py)ì— ë‚˜ì™€ìˆëŠ” ë²„ì ¼ì˜ DistributedEvalSamplerë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ê²ƒì„ ì‚¬ìš©í•˜ë©´ extra padding ì—†ì´ print(test_sampler.total_size)ì˜ ê²°ê³¼ëŠ” ì •í™•íˆ 1ë§Œì´ ëœë‹¤.

# 2. HuggingFace Accelerate

[[HuggingFace Accelerate]](https://huggingface.co/docs/accelerate/index)

>ğŸ¤— Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.

```python
from accelerate import Accelerator
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR

class BasicNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.act = F.relu

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.act(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.act(self.fc1(x))
        x = self.dropout2(x)
        output = self.fc2(x) 
        return output

def main():
    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307), (0.3081))
    ])

    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)
    test_dset = datasets.MNIST('data', train=False, transform=transform)

    training_dataloader = DataLoader(train_dset, shuffle=True, batch_size=64)
    validation_datalaoder = DataLoader(test_dset, shuffle=False, batch_size=64)

    model = BasicNet()

    optimizer = optim.AdamW(model.parameters(), lr=1e-3)

    scheduler = StepLR(optimizer=optimizer, step_size=30, gamma=0.1)

    accelerator = Accelerator()

    model, optimizer, training_dataloader, scheduler = accelerator.prepare(
        model, optimizer, training_dataloader, scheduler
    )

    model.train()
    criterion = torch.nn.CrossEntropyLoss()
    # train - 1 epoch
    for batch in training_dataloader:
        optimizer.zero_grad()
        inputs, targets = batch
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        accelerator.backward(loss)
        optimizer.step()
    scheduler.step()

    validation_datalaoder = accelerator.prepare(validation_datalaoder)
    # test - 1 epoch
    model.eval()
    correct = 0
    with torch.no_grad():
        for batch in validation_datalaoder:
            inputs, targets = batch
            outputs = model(inputs)
            pred = outputs.argmax(dim=1, keepdim=True)
            # Gathers tensor and potentially drops duplicates in the last batch if on a distributed system.
            all_preds, all_targets = accelerator.gather_for_metrics((pred, targets))
            if accelerator.is_main_process:
                correct += all_preds.eq(all_targets.view_as(all_preds)).sum().item()
    if accelerator.is_main_process:            
        accelerator.print(f'Accuracy: {100. * correct / len(validation_datalaoder.dataset)}')   

if __name__ == "__main__":
    main()
```

ì´í›„ í„°ë¯¸ë„ ì°½ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•´ì£¼ë©´ ëœë‹¤.

>CUDA_VISIBLE_DEVICES="6,7" accelerate launch --config_file ./my_config_file.yaml example_script.py

ì—¬ê¸°ì„œ my_config_file.yamlì€ Multi-GPU training ê´€ë ¨ ì„¤ì •ë“¤ì„ ì§€ì •í•´ì£¼ëŠ” íŒŒì¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•´ì£¼ì—ˆë‹¤. ê³µì‹ ë¬¸ì„œì— ë‚˜ì™€ìˆëŠ” ì‚¬í•­ ê·¸ëŒ€ë¡œì´ë©°, FP16 Mixed-precisionì´ í™œìš©ë˜ì—ˆë‹¤.

```plaintext
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MULTI_GPU
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 2
use_cpu: false
```

## Cautions
1. .cuda(), .to(device) ë“±ì„ ì—†ì• ê³  accelerator.prepare()ì— ëª¨ë‘ ë³´ë‚´ë©´ ëœë‹¤.
2. loss.backward()ê°€ ì•„ë‹ˆë¼ accelerator.backward(loss)ë¡œ ë³´ë‚´ì¤€ë‹¤.
3. DDPì™€ ìœ ì‚¬í•˜ê²Œ, evaluation ì‹œ ì „ì²´ë¥¼ ëª¨ë‘ ëª¨ìœ¼ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. accelerator.gather() í˜¹ì€ ìœ„ ì½”ë“œì²˜ëŸ¼ accelerator.gather_for_metrics() ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•œë‹¤. DDPë³´ë‹¤ ë‚˜ì€ ì ì€, DDPëŠ” extra sample padding ë•Œë¬¸ì— ë˜ ë‹¤ë¥¸ í›„ì† ì²˜ë¦¬ê°€ í•„ìš”í–ˆìœ¼ë‚˜, acceleratorì˜ ê²½ìš° ìë™ìœ¼ë¡œ í•´ë‹¹ ì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤.
4. accelerator.is_main_process ëŠ” local_rank==0ê³¼ ë™ì¼í•˜ë‹¤. ë”± í•œ ë²ˆ ì‹¤í–‰ëœë‹¤.
5. ê³µì‹ë¬¸ì„œì—ë„ ë‚˜ì™€ìˆë“¯ì´, accelerate configë¥¼ accelerate launch ì´ì „ì— ì‹¤í–‰í•˜ê¸°ë¥¼ highly ê¶Œì¥í•˜ê³  ìˆë‹¤. (ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Accelerateê°€ ìë™ìœ¼ë¡œ ë‹¤ í•´ë²„ë¦°ë‹¤ê³  ê²½ê³ í•˜ê³  ìˆë‹¤.) ê·¸ë˜ì„œ ìœ„ ì‹¤í–‰ ëª…ë ¹ì–´ ì²˜ëŸ¼ config_fileì„ ë”°ë¡œ ì €ì¥í•´ë‘ê¸°ë¥¼ ê¶Œì¥í•˜ëŠ” ê²ƒ ê°™ë‹¤. hyperparameter ì§€ì •í•˜ëŠ” yaml íŒŒì¼ í•˜ë‚˜, Multi-gpu training option ì§€ì •í•˜ëŠ” yaml íŒŒì¼ í•˜ë‚˜ ì´ë ‡ê²Œ ì‚¬ìš©í•˜ë©´ ë  ê²ƒ ê°™ë‹¤. 
6. torchrun ì¨ë„ ëœë‹¤ê³  í•œë‹¤.

# Summary
[[Multi GPU Training Examples]](https://github.com/GyoukChu/Code-Review/tree/main/Multi-GPU)

# Reference

## Websites

[ì‚¬ì´íŠ¸ ì¶œì²˜ 1] (https://huggingface.co/docs/accelerate/main/en/package_reference/accelerator)

[ì‚¬ì´íŠ¸ ì¶œì²˜ 2] (https://huggingface.co/blog/pytorch-ddp-accelerate-transformers)

[ì‚¬ì´íŠ¸ ì¶œì²˜ 3] (https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py)

[ì‚¬ì´íŠ¸ ì¶œì²˜ 4] (https://velog.io/@mmsori/PyTorch-Distributed-Sampler-in-evaluation)

[ì‚¬ì´íŠ¸ ì¶œì²˜ 5] (https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/data/sampler.py)

## Papers
