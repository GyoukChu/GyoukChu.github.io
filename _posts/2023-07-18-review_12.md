---
layout: single
title:  "[Review] Diffusion Transformer (DiT)"
categories: review
tag: [review, Generative Model]
author_profile: false
---

Diffusion Model의 기본 Architecture는 DDPM 때 부터 U-Net을 주로 써왔다. 그러나, "굳이 U-Net?"이라는 의문이 들 수 있기에. Transformer를 사용한 Diffusion Transformer, DiT 논문을 살펴보자. 정확한 논문 이름은 Scalable Diffusion Models with Transformers.

# 1. Introduction

## Transformer & ViT

[[My Transformer Review]](https://gyoukchu.github.io/review/review_1/#3-model-architecture)

Vision Transformer(ViT)에 대한 내용을 아직 글을 작성하지는 않았지만, 워낙에 유명하기도 하고 Transformer 구조와 크게 다른 것이 없기에. 일단 큰 틀은 아래 사진과 같다.

![ViT]({{site.url}}/images/review/DiT/1.png)

논문 제목처럼 Image를 PxP patch로 나눈 뒤 이 각각의 patch를 Transformer에서 하나의 단어로 취급하는 구조이다. Position Embedding도 Transformer에서 각 단어가 문장에서 몇 번째 위치인지에 대한 정보를 담고 있듯이 ViT에서는 각 patch가 image의 어떤 위치에 있었는지에 대한 정보를 담고 있고, 학습해도 되고 sinusoidal embedding으로 encoding해도 된다. BERT에서 idea를 가져와서 BERT에서의 CLS 토큰 처럼 추가로 학습할 0번 토큰을 넣어주고 있다.

ViT 모델 구조에서 hyperparameter로는 Transformer Encoder Layer 개수, Hidden dimension size, Attention 시 head 수, MLP size에 따라 Base, Large, Huge 모델로 나뉜다.

## DM / LDM

[[My Diffusion Review]](https://gyoukchu.github.io/review/review_2/#2-dpm)

Forward Process $$q(X_{t}\mid X_{0})=N(X_{t}:\sqrt{\overline{\alpha_{t}}}X_{0},(1-\overline{\alpha_{t}})I)\;where\;\alpha_{t}=1-\beta_{t},\overline{\alpha_{t}}=\prod_{i=1}^{t}\alpha_{i}$$를 통해 initial data에 noise를 단계적으로 넣어주고, reverse process $$q(X_{t-1}\mid X_{t})$$의 분포를 따라 noise를 제거해주면 되기에 이를 $$p_{\theta}(X_{t-1}\mid X_{t})=N(\mu_{\theta}(x_{t}),\Sigma_{\theta}(x_{t}))$$로 네트워크를 통해 학습한다. 학습할 때 Loss는 $$\begin{align*}V_{vlb}&:=L_{0}+L_{1}+...+L_{T-1}+L_{T} \\ L_{0}&:=-log\,p_{\theta}(x_{0}\mid x_{1}) \\ L_{t-1}&:=D_{KL}(q(x_{t-1}\mid x_{t},x_{0}) \,||\,p_{\theta}(x_{t-1}\mid x_{t})) \\ L_{T}&:=D_{KL}(q(x_{T}\mid x_{0})\,||\,p(x_{T}))\end{align*}$$로, 여기서 DDPM에서는 $$p_{\theta}(X_{t-1}\mid X_{t})$$의 분산은 학습하지 않고 평균을 "더해지는 noise를 학습함"으로써 학습하였다: $$\mu_{\theta}(x_{t},t)=\frac{1}{\alpha_{t}}(x_{t}-\frac{\beta_{t}}{\sqrt{1-\overline{\alpha_{t}}}} \epsilon_{\theta}(x_{t},t))$$ 또한 Loss를 $$L_{simple}=\mathbb{E}_{t,x_{0},\epsilon}[\left \| \epsilon-\epsilon_{\theta}(x_{t},t) \right \|^{2}]$$로 간단히 하였다.

Improved DDPM 논문에서 noise scheduling만 다루었었는데, 해당 논문에서는 평균뿐만 아니라 분산 또한 학습하는 것을 다루고 있다. simple loss term만으로 평균(정확히는 noise)을 학습하고, stop-gradient를 이용해 full loss term으로 분산을 학습하는 방식이다. Diffusion Transformer 논문에서도 동일하게 이와 같은 방식으로 학습을 진행하였다.

Latent Diffusion(Stable diffusion) Model도 Diffuison 리뷰 글에서 다루었는데, Diffusion Transformer 논문도 LDM의 방식을 그대로 따른다. 1/8배 downsample하는 pre-trained VAE encoder를 거친 뒤 latent 단에서의 diffusion을 학습하고, 이를 다시 pre-trained VAE decoder를 거치는 방식이다. 

## Classifier Free Guidance

역시 Diffusion Review 글에서 잠깐 다루었었는데, $\varepsilon_{\theta}(x_{t})-s\nabla_{x_{t}}logp_{\phi}(y \mid x_{t})$로 나타나는 Classifier Guidance에서 추가적인 Classifier 학습 없이 동일한 효과를 낼 수 있는 방법이 CFG였다. Bayes Rule에 따라 $$\nabla_{x_{t}}logp_{\phi}(y \mid x_{t})\propto \nabla_{x_{t}}logp_{\phi}(x_{t} \mid y)-\nabla_{x_{t}}logp_{\phi}(x_{t})$$를 이용한 것으로, $$\hat \varepsilon_{\theta}(x_{t},c)=\varepsilon_{\theta}(x_{t},\phi)-s\nabla_{x_{t}}logp_{\phi}(y \mid x_{t})\propto \varepsilon_{\theta}(x_{t},\phi)+s(\varepsilon_{\theta}(x_{t},c)-\varepsilon_{\theta}(x_{t},\phi))$$가 되겠다. (이 때 $\phi$는 빈 text embedding)

Diffusion Transformer 에서도 마찬가지로 Classifier Free Guidance를 통한 Class conditioned Image generation 결과를 보여주고 있다.

## Architecture Complexity - Gflops

Architecture가 얼마나 복잡한지 나타내는 직관적인 지표로는 모델의 parameter 개수가 떠오를 수 있겠지만, 이는 complexity로써의 지표에 적합하지 않다. 가령 예를 들면 ViT-Base 모델에서 patch size 16과 patch size 32는 모델 파라미터 수는 같지만, GFLOPs가 거의 4배 차이가 난다. 여기서 Gflops는 floating point operations, 부동소수점 연산량으로 곱하기 더하기 등의 연산량 총 개수를 뜻한다. 그래서 Diffusion Transformer에서는 이 Gflops 수치를 통해 모델이 복잡할수록 성능이 좋아진다는 것을 보여주고 있다. 물론 막연하게 Gflops만 올리는 것이 아니라 최소한으로 올리면서도 성능은 좋은 방식을 제시하는데, 이제 이에 대해서 알아보자.

# 2. Design

## Patchify

![Patchify]({{site.url}}/images/review/DiT/2.png)

## Block design

![DiTBlock]({{site.url}}/images/review/DiT/3.png)

## Model size

![DiT Size]({{site.url}}/images/review/DiT/4.png)

# 3. Experiment Results

![DiT Result]({{site.url}}/images/review/DiT/5.png)

## Ablation Studies

![DiT Ablation 1/2]({{site.url}}/images/review/DiT/6.png)

![DiT Ablation 3/4]({{site.url}}/images/review/DiT/7.png)

# Summary

# Reference

## Websites

[사이트 출처 1] 

## Papers

[1] Peebles, W., & Xie, S. (2022). Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748.

[2] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.