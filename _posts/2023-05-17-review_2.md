---
layout: single
title:  "[Review] Diffusion Models"
categories: review
tag: [review, DDPM]
author_profile: false
---

Diffusion Model의 기본이라 불리우는 4~5개의 논문에 대한 강의를 보고 정리해보자. 이후에는 AnoDDPM 같이 Anomaly detection에 diffusion model을 활용한 사례들을 살펴볼 계획이다. ~~게을러지지 않는다면~~
1. Denoising diffusion probabilistic models (DDPM), NeurIPS 2020
2. Denoising diffusion implicit models (DDIM), ICLR 2021
3. Diffusion Models Beat GANs on Image Synthesis, NeurIPS 2021
4. Tackling the Generative Learning Trilemma with Denoising Diffusion GANs, ICLR 2022 Spotlight
5. High-Resolution Image Synthesis with Latent Diffusion Models (Stable diffusion), CVPR 2022

# 1. Introduction
## VAE vs. GAN vs. Diffusion
![Comparison1]({{site.url}}/images/review/Diffusion/1.png)
![Comparison2]({{site.url}}/images/review/Diffusion/2.png)

4번 논문에서 각각의 장단점에 대해 잘 설명해둔, 위 두번째 그림이 있다.
GAN은 high quality sample과 빠른 sampling에 장점이 있지만, 다양한 샘플, training distribution을 잘 따라가지 못한다.
VAE는 다양한 sample과 빠른 sampling에 장점이 있지만, blur한 sample이 나온다.
Diffusion model은 high quality도 되고, 다양한 sample들도 잘 만들지만, sampling이 느리다.
그래서 논문 제목처럼 일종의 trilemma가 있다는 것 같다. 제목 참 잘 지었네...

# 2. DPM
사실 Diffusion model의 원조는 2015년 발표된 "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"라는 논문이라고 한다. 초창기에는 diffusion process를 통해 Gaussian distribution 같은 well-known distribution으로 부터 target data distribution으로 변환하는 일종의 markov chain을 학습시켜 distribution을 구하고자 하는 모델이다. 

Discrete Markov Chain의 property는 잘 알려져있다 싶이, 시간 t+1에서의 확률은 오직 현재 t에만 의존하는 성질이 있다. 즉, Random process가 memoryless property (Markov property라고 했었던 거 같기도 하고)를 가진다.

아무튼 Diffusion model은 기본적으로 data에 임의의 noise를 더해준 후(forward/diffusion process), noise를 제거하는 과정(reverse/denoising process)을 학습하는 모델이다. GAN은 noise로부터 한번에 이미지를 만들지만, Diffusion은 noise를 넣고 빼는 과정으로부터 이미지를 복원하는 거라 살짝 다르다. 여기서 forward process는 $q(X_{t}\mid X_{t-1})$ 확률 (이전 step에서 다음 step의 분포를 구하는 것)을 구하는 과정인데, 이는 정해져있는 것이고 관건은 reverse process. $q(X_{t-1}\mid X_{t})$ 확률 (현재 step에서 이전 step의 분포를 구하는 것)을 구하는 과정인데, 최종적으로 diffusion model의 목표는 이 분포를 학습을 통해 가능하게 만들고자 하는 것이다.

네트워크도 간단한데, time step을 embedding으로 (FC layer를 거친 후) U-net structure network에 넣어줘서 image를 통과시키면 된다.

## Forward Process
$$q(X_{t}\mid X_{t-1})=N(X_{t}:\sqrt{1-\beta _{t}}X_{t-1}, \beta _{t}*I)$$

즉, $x_{t}=\sqrt{1-\beta_{t}}*x_{t-1}+\sqrt{\beta_{t}}*\varepsilon$으로 variance를 1로 맞춰주면서 딥러닝 학습이 가능하도록 작은 노이즈를 더해주는, reparameterization trick을 이용한다. input도 normalizaion을 통해 variance가 1이 되도록 맞춰줘야 된다. (안 맞추고 하면 어떻게 될지 궁금하긴 하다) 이 때, $\beta_{t}$는 학습을 할 수도 있겠지만 미리 정해진 상수로 정의해서 forward process에는 학습이 전혀 필요하지 않게 된다. (논문에서는 학습을 해봤는데 상수로 둔 거랑 별 차이가 없었다고 말했다) 또한 $\beta_{t}$의 scheduling을 1e-4~0.02로 점차 증가시키는데 (linear, sigmoid, cosine 등 여러가지 scheduling이 또 가능하겠다. linear는 너무 빠르게 noise로 변하는 문제가 있어서 cosine 등의 scheduling을 도입했다고 한다.), 처음에는 noise를 조금씩 더하다가 점차 많이 더해가는 느낌이다.

한 방에 적으면, $$q(X_{t}\mid X_{0})=N(X_{t}:\sqrt{\overline{\alpha_{t}}}X_{0},(1-\overline{\alpha_{t}})*I)\;where\;\alpha_{t}=1-\beta_{t},\overline{\alpha_{t}}=\prod_{i=1}^{t}\alpha_{i}$$
즉, 학습할 건 없고 한 방에 계산이 가능하다는 거다.

## Reverse Process
![reverse1]({{site.url}}/images/review/Diffusion/3.png)

위 그림처럼 VAE는 1개의 latent vector로부터 원래 것을 복원하는 것이라면, diffusion model은 여러개의 step을 통해 만들어낸 여러개의 latent variable들로 부터 markov chain process를 통해 원래 것을 복원하는 것이다. 앞서 말했던 우리의 목표, $q(X_{t-1}\mid X_{t})$ 확률을 알기 위해 이와 유사한 $p_{\theta}(X_{t-1}\mid X_{t})$를 파라미터를 통해 학습하게 된다.

![reverse2]({{site.url}}/images/review/Diffusion/4.png)

VAE의 loss를 recall해보면 그 복잡한 ELBO하면서 식 전개 해서 나온 loss 함수는 encoder에 KL-divergence로 regularization을 하는 term 하나와, decoder로부터 L1 혹은 L2로 reconstruction loss를 하는 term 이렇게 구성되어 있다.

Diffusion도 비슷하게 식 전개를 한다고 한다. 자세히 보기엔 수식이 많아서 좀 겁난다... 아무튼 KL-divergence로 regularization을 하는 term 하나와, reconstruction loss를 하는 term 두 개가 마찬가지로 포함되어 있다. 여기에 추가로 latent가 여러개라 각 step마다 KL-divergence로 regularization을 하는 term을 더한 항, denoising process term이 추가된다. 식만 보자면 우리가 denoising 하는 $p_{\theta}(X_{t-1}\mid X_{t})$ 분포와, $q(X_{t-1}\mid X_{t})$ 실제 이 분포가 닮도록 해주는 것이다.

### DDPM
DDPM의 공헌은 이 loss term에서 시작된다. 우선 첫 번째 Regularization term을 고려하지 않는다. (상수 취급하는 것과 똑같은 말이다) DDPM 논문의 Section 3.1에서는 이렇게 언급하고 있다.
>We ignore the fact that the forward process variances βt are learnable by reparameterization and
instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate
posterior q has no learnable parameters, so LT is a constant during training and can be ignored.

느낌상으로는 $p_{\theta}(X_{t})$는 충분한 forward process를 거쳤기에 $q(X_{t}\mid X_{0})$ 와 같은 Guassian distribution을 이미 잘 따른다고 가정해도 충분하다 뭐 이런 얘기인데, Section 4를 읽어보면 [-1, 1]의 범위로 scaled 된 data에서 $\beta _{t}$를 1e-4~0.02범위로 설정하면 reverse process와 forward process가 거의 같은 functional form을 가져서 이 loss term이 1e-5 bits per dimension, 매우 작아진다고 한다. 그래서 무시하는 건가보다.

그리고 맨 마지막 reconstruction term을 고려하지 않는다. "The variational bound is a lossless codelength of discrete data"라고 section 3.3에 언급되어 있는데, 그냥 저 term이 처음 $\beta _{0}$와 비슷한데 매우 작아서 무시하는 거라고 생각하고 싶다. ~~잘 이해를 못하겠다~~

# 3. DDPM
## Training

## Implementation
# 4. DDIM

# 5. Guided Diffusion

# 6. DDGAN

# Reference

## Websites
[사이트 출처 1] https://www.youtube.com/watch?v=Z8WWriIh1PU

## Papers
[1] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33, 6840-6851.
[2] Song, J., Meng, C., & Ermon, S. (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502.
[3] Dhariwal, P., & Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34, 8780-8794.
[4] Xiao, Z., Kreis, K., & Vahdat, A. (2021). Tackling the generative learning trilemma with denoising diffusion GANs. arXiv preprint arXiv:2112.07804.
[5] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10684-10695).

