---
layout: single
title:  "[Review] BERT"
categories: review
tag: [review, BERT, NLP]
author_profile: false
---

Transformer 모델을 살펴보았으니, 이젠 본격적으로 BERT와 GPT을 살펴볼 차례이다. 이번에는 BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding<sup>[1]</sup> 논문을 읽고 리뷰해보자. 다음에는 GPT-1,2,3,4(4는 technical report인 게 아쉽지만)를 순서대로 리뷰해보는 시간을.

# 1. Introduction
BERT가 arxiv 기준 ver 1.이 2018년 10월 11일, GPT가 OpenAI 사이트 기준 2018년 6월 11일으로 나와있다. BERT 논문에서도 GPT와 어떤 점이 다른 지 여러 번 얘기하는 걸 보아 살짝 뒤늦게 나온 것이 맞는 것 같다. NLP 분야에서 Transformer가 나온 뒤에 이 구조를 응용하고자 하는 시도가 그 때 당시 확실히 많았던 것 같다. 확실히 transformer 모델이 커서 학습이 느린데다가, inductive bias가 CNN보다 작아 학습을 위해 dataset이 커야 되서 이것들이 단점으로 작용해 개선하고자 하는 노력이 있었던 것으로 보인다.

여하튼, NLP에서 task를 수행하는 데에 있어서 Feature-based approach와 fine-tuning approach 두 가지를 Related Work로 소개하고 있다. Feature-based unsupervised learning은 말 그대로 word-embedding, 더 나아가 paragraph-embedding, sentence-embedding representation을 학습하는 것으로, 실제 문장처럼 left-to-right representation 뿐만 아니라 right-to-left representation을 모두 학습한 ELMo가 당시 SOTA였다. Fine-tuning method는 잘 알려져있듯이, unlabeled text로부터 pre-trained된 contextual token representation encoder를 supervised downstream task의 initial parameter로 활용하여 fine-tuning 하는 것이다. Left-to-right LM과 auto-encoder objective로 pre-training한 GPT-1을 SOTA로 소개하고 있다.

그러나 ELMo와 GPT-1 모두 LTR, unidirectional LM이라 Transformer의 bidirectional, self-attention mechanism을 안 쓰고 있다. 이 특징이 pre-trained representation을 학습하는데 있어서 성능을 저해하는 문제점이기에, BERT는 이런 self-attention을 잘 활용하여 bidirectional model을 구성하였다. "GPT-1도 Transformer를 쓴다면서 self-attention이 있는거 아닌가?"하는 의문이 들 수도 있지만, GPT-1은 Transformer Decoder block의 Masked MHSA mechanism을 사용하는데, 이는 Transformer Review 때도 다루었지만 특정 시점 t에서의 output이 이전 시점 t-1, ...에 의존하는, 그래서 LTR-LM 특징을 갖게 된다. *GPT Review에서 자세히*

![Comparison]({{site.url}}/images/review/BERT/1.png)

# 2. BERT

## Architecture
BERT는 앞에 잠깐 말했듯이 Bidirectional Encoder Representations from Transformer의 약자로, 말 그대로 Transformer Encoder를 Bidirectional로 multi-layer 쌓은 구조이다.

## Overall Process
![Overall]({{site.url}}/images/review/BERT/2.png)

## BertTokenizer
![Tokenizer]({{site.url}}/images/review/BERT/3.png)

[HuggingFace-BertTokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer)

## Pre-training

### Maksed LM(MLM)

### Next Sentence Prediction(NSP)

## Fine-tuning

## Ablation Study

# 3. Experiments

## GLUE Benchmark

![GLUE_Summary]({{site.url}}/images/review/BERT/4.png)

그렇다면 2023년 6월 28일 기준 top은 무엇일까? gluebenchmark leaderboard를 살펴보자.

![GLUE_BERT]({{site.url}}/images/review/BERT/5.png)
논문에 있는 BERT large 모델은 현재 48등이고, 놀라운 점은 1등도 "XY-LENT: X-Y bitext enhanced Language ENcodings using Transformers", 결국 Transformer 구조라는 것과 전체적으로 BERT의 variant들이 매우매우 많다는 점. 논문에서는 WNLI set이 당시에 뭐 문제가 있었는지 그 결과를 제외하고 전체 결과를 주고 있다.

이 외에도 SQuAD (Standford Question Answering Dataset, task: to predict the answer text span in the passage), SWAG (Situations With Adversarial Generations, task: to choose the most plausible continuation among 4 choices) 등에서 SOTA를 기록했다. ~~그 때 당시~~

# Reference

## Websites
[사이트 출처 1] (https://huffon.github.io/2019/11/16/glue/)

[사이트 출처 2] (https://gluebenchmark.com/leaderboard/)

## Papers

[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.


