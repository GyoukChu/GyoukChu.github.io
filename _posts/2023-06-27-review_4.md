---
layout: single
title:  "[Review] BERT"
categories: review
tag: [review, BERT, NLP]
author_profile: false
---

Transformer 모델을 살펴보았으니, 이젠 본격적으로 BERT와 GPT을 살펴볼 차례이다. 이번에는 BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding<sup>[1]</sup> 논문을 읽고 리뷰해보자. 다음에는 GPT-1,2,3,4(4는 technical report인 게 아쉽지만)를 순서대로 리뷰해보는 시간을.

# Reference

## Websites
[사이트 출처 1] 

## Papers

[1] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.


