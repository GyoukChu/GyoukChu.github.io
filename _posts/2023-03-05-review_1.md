---
layout: single
title:  "Transformer(2017) 리뷰"
categories: review
tag: [review, Transformer]
author_profile: false
---

ChatGPT에게 Transformer 모델에 대해 설명을 요청한 결과, 내용을 굉장히 잘 정리해서 알려주고 있다!
> "Transformer 모델은 자연어 처리 분야에서 최근에 개발된 딥러닝 모델 중 하나로, 특히 기계 번역, 자연어 이해, 질의응답 등 다양한 자연어 처리 태스크에서 높은 성능을 보이며 널리 사용되고 있습니다.
Transformer 모델은 기존의 RNN(Recurrent Neural Network)이나 CNN(Convolutional Neural Network)과는 달리, 어떠한 시퀀스 정보도 전달하지 않으면서 장기 의존성(Long-term dependency) 문제를 해결할 수 있는 모델입니다. 이를 가능하게 하는 것은 Self-Attention(자기 주의) 메커니즘입니다.

> Self-Attention은 입력 시퀀스의 각 단어들이 서로 어떤 관련이 있는지를 파악하여, 높은 상관관계를 가진 단어들을 더 중요하게 취급하고, 이를 통해 입력 시퀀스 전체를 동시에 처리할 수 있습니다. 이를 통해 Transformer 모델은 장기 의존성을 포함한 매우 긴 시퀀스에서도 높은 성능을 발휘할 수 있습니다.
또한 Transformer 모델은 인코더-디코더(Encoder-Decoder) 구조를 사용하여 기계 번역, 질의응답 등의 태스크를 수행합니다. 인코더는 입력 문장을 벡터 형태로 변환하고, 디코더는 이 벡터를 바탕으로 출력 문장을 생성합니다. 이 때, 인코더와 디코더는 각각 여러 층으로 구성되어 있으며, 각 층은 여러 개의 Self-Attention과 Feed-Forward 네트워크를 포함합니다.

> Transformer 모델은 이러한 구조와 Self-Attention 메커니즘을 통해 자연어 처리 분야에서 높은 성능을 보이고 있으며, 이를 기반으로 한 BERT, GPT 등의 다양한 변형 모델들이 개발되어 사용되고 있습니다."

"Attention Is All You Need(Vaswani et al., 2017)[1] 논문을 직접 읽어보면서 자세한 사항들을 알아보자.

# 1. Introduction
## Existing Model
### LSTM (Long Short-Term Memory)[2]
![LSTM]({{site.url}}/images/review/Transformer/LSTM.png)

Speech나 Video와 같은 데이터를 처리함에 있어서 기존에는 주로 RNN을 사용하였다.
$$\left\{\begin{matrix}
h^{(t)}=f(h^{(t-1)},x^{(t)}) \\ 
y^{(t)}=g(h^{(t)}) 
\end{matrix}\right.$$
식에서도 알 수 있듯이, RNN은 $t$에서의 ouput을 결정하는 hidden state $h^{t}$가 이전 layer의 hidden state $h^{t-1}$에만 의존한다. ~~*(물론 input도 있긴 하지만)*~~ 그래서 Layer가 길어질 수록 old observation은 forgotten되는 문제점이 있다. 추가적으로 SGD가 매우 noisy해져서 vanishing gradient 문제, 혹은 gradient가 explode 될 수 있다.

LSTM은 RNN의 장기의존성(Long-range dependency)를 개선한 모델로, "과거의 기억을 까먹는 것 또한 학습해야 된다"라고 이해하면 좋다. 그림에서 볼 수 있듯이, 하나의 모듈에 3개의 Gate를 거치게 된다. 가장 핵심적인 부분은 Cell state로, 그림에서 가장 위 쪽, $C_{t-1}$에서 $C_{t}$로 이어지는 수평선에 해당한다.

첫 번째 Gate(Forget Gate)는 sigmoid와 pointwise-multiplication으로 이루어진, "Cell state로부터 어떤 정보를 버릴래?"에 대한 것을 결정한다. sigmoid 값이 0 근방이라면 forget, 1 근방이라면 remember가 된다. 

두 번째 Gate(Input Gate)는 sigmoid와 tanh의 mulitplication을 더하는, "Cell state에 어떤 정보를 더할래?"에 대한 것을 결정한다. sigmoid를 거친 값은 "어떤 indices를 쓸래?", tanh를 거친 값은 "해당 indices에 뭘 쓸래?"로 해석할 수 있다. multiplication으로 인해 **해당**이 붙었다 생각하면 되고, 이것이 Cell state에 더해지게 된다. 더해지는 과정을, "앞에서 정보를 버렸으니, 이번엔 선택된 정보를 써보자"로 이해할 수 있겠다.

마지막 Gate는 sigmoid와, tanh를 거친 Cell state의 multiplication으로 이루어진, "최종 output으로 뭘 할래?"에 대한 것을 결정한다. 

### GRU (Gated Recurrent Unit)[3]
![GRU]({{site.url}}/images/review/Transformer/GRU.png)
~~한국인의 작품이다!~~

Transformer 논문에서 소개된, LSTM의 변형 작 중 하나이다. 자세히 읽어보지는 않았지만, 그림만 봤을 때는 Cell state가 hidden state와 합쳐져있고, Forget gate와 Input gate가 합쳐진 변형 모델과 비슷한 듯 다르며, 또 여러가지 변경 및 개선사항이 있는 것 같다. 결과적으로 기존 LSTM보다 단순한 구조(그림만 보면 단순해 보이지는 않지만)를 가져서 인기가 있다고 한다.

### Encoder-Decoder
![seq2seq]({{site.url}}/images/review/Transformer/seq2seq.gif)

Seq2seq 모델은 Encoder를 통해 input을 Context vector로 표현하고, Decoder를 통해 Context vector를 output으로 변환하는 구조이다. 이 때 Context vector의 크기가 고정되어 있어, input이 길 경우 성능이 좋지 않아지게 되는데... 이를 개선하는 법이 그림에도 나와있는, 그리고 Transformer 논문에서도 중요 사항이 되는 Attention Mechanism이다.

위 문제점을 개선하기 위해서 고안한 아이디어는 "hidden state들 중에서, 현재 $t$에서의 ouput을 결정하는 가장 중요한 hidden state는 무엇일까?"이다. 
![attention]({{site.url}}/images/review/Transformer/attention.png)

결국 그림에서의 attention weight를 구해서, context vector를 hidden vector의 weighted sum으로 표현하게 된다. 여기서 attention weight를 구하는 방법은 여러가지가 제안되어 있었는데, 자세히 살펴보지는 않겠다. 
![attention2]({{site.url}}/images/review/Transformer/attention_variant.png)

논문에서도 위의 모델이 sequence modeling과 transduction problem에서 SOTA임을 말하며 간단히 소개하고 있고, 자신들의 모델인 Transformer는 Recurrence를 회피(eschew)하고 attention mechanism에 entirely 의존하여 global dependency를 이끌었다고 말하고 있다. Attention을 RNN/LSTM과 함께 활용한 모델이 대부분이라, Transformer는 이에 벗어나서 이제 sequential data를 **병렬화(Parallelization)** 처리할 수 있게 된 것이다.
>"The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs."

# 2. Background

바로 위에서 말한, sequential data를 병렬처리하는 것과 관련하여 한 번 더 얘기를 하고 있다. Transformer 이전에도 ~~당연히~~ sequential computation cost를 줄이기 위해 여러 노력들을 했는데, 논문에서 소개한 것은 Extended Neural GPU (Kaiser et al., 2016), ByteNet (Kalchbrenner et al., 2017), 그리고 ConvS2S (Gehring et al., 2017)이다. 3가지 모델 모두 CNN을 기반으로 sequential data에서 hidden representation을 계산하는데, 두 개의 임의의 input 또는 output position에서 신호를 연관짓는데 operation 수가 position간 거리에 대해 ConvS2S는 linear하게, ByteNet은 logarithmic하게 증가한다고 한다. (해당 모델들을 자세히 본 적이 없어서 모름) 이는 멀리 떨어진 position간의 의존성을 학습하기에 부적합한 사항인데, Transformer에서는 operation 수가 constant여서 효과적이다고 말하고 있다! (그 이유가 Multi-Head Attention이라는데, 이는 3-2 Attention에서 다뤄보자.)

Self-attention은 single sequence의 서로 다른 position들에 대한 attention mechanism인데, 해당 내용은 논문에서 또 자세히 설명되고 있으니 Pass.

End-to-end memory network는 말 그대로 end-to-end로 학습된, Memory Network이다. Memory network는, encoder-decoder 구조에서 context vector가 입력이 매우 길 때 입력의 앞부분을 반영하기 힘든 문제를 해결하기 위해 제안된 것으로, 메모리에 저장할 수 있는 만큼 최대한 각 단계에서 hidden state를 저장해 써 먹는 것이다.

아무튼, Transformer는 sequence-aligned RNN이나 Conv를 쓰지 않고, input의 representation을 오직 self-attention mechanism에만 의존하는 최초의 변환 모델이다.

# 3. Model Architecture

## 3-1. Encoder and Decoder Stacks

## 3-2. Attention

### 3-2-1. Scaled Dot-Product Attention

### 3-2-2. Multi-Head Attention

### 3-2-3. Applications of Attention in our Model

## 3-3. Position-wise Feed-Forward Networks

## 3-4. Embeddings and Softmax

## 3-5. Positional Encoding

# 4. Why Self-Attention

# 5. Training

# 6. Results

# 7. Conclusion

# Reference

## Websites
[사이트 출처 1](https://wikidocs.net/31379) (https://wikidocs.net/31379)
[사이트 출처 2](https://tunz.kr/post/4) (https://tunz.kr/post/4)

## Images
[그림 출처 1] 2022 Fall EE488(B) DL for CV Lecture Note (by Prof. Jung)
[그림 출처 2](https://excelsior-cjh.tistory.com/185) (https://excelsior-cjh.tistory.com/185)
[그림 출처 3](https://google.github.io/seq2seq/) (https://google.github.io/seq2seq/)

## Papers
*글에서 나오는 내용 순서대로 참고한 논문이 작성됨*


[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

[2] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.

[3] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.

[4] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.